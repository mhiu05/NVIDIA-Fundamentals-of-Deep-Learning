{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hopefully, you've learned some valuable skills along the way and had fun doing it. Now it's time to put those skills to the test. In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning. Once you have trained the model to be at least 92% accurate on the validation dataset, save your model, and then assess its accuracy. Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:53:45.242416Z",
     "start_time": "2025-08-09T04:53:28.054509Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will train a model to recognize fresh and rotten fruits. The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification), a great place to go if you're interested in starting a project after this class. The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Load ImageNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to start with a model pretrained on ImageNet. Load the model with the correct weights. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. If you need a reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where we implemented transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:02.261674Z",
     "start_time": "2025-08-09T04:54:00.047343Z"
    }
   },
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we suggest freezing the base model, as done in [notebook 05b](05b_presidential_doggy_door.ipynb). This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:05.346742Z",
     "start_time": "2025-08-09T04:54:05.339588Z"
    }
   },
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add layers to the pretrained model. [Notebook 05b](05b_presidential_doggy_door.ipynb) can be used as a guide. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit.\n",
    "\n",
    "The later layers of a model become more specific to the data the model trained on. Since we want the more general learnings from VGG, we can select parts of it, like so:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:11.900510Z",
     "start_time": "2025-08-09T04:54:11.892842Z"
    }
   },
   "source": [
    "vgg_model.classifier[0:3]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've taken what we've wanted from VGG16, then we can add our own modifications. No matter what additional modules we add, we still need to end with one value for each output."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:13.447395Z",
     "start_time": "2025-08-09T04:54:13.427473Z"
    }
   },
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "my_model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (2): Flatten(start_dim=1, end_dim=-1)\n",
       "  (3): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (4): Linear(in_features=4096, out_features=500, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=500, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. We have 6 classes, so which loss function should we use?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:17.328110Z",
     "start_time": "2025-08-09T04:54:15.764917Z"
    }
   },
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = torch.compile(my_model.to(device))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess our input images, we will use the transforms included with the VGG16 weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:18.222025Z",
     "start_time": "2025-08-09T04:54:18.216015Z"
    }
   },
   "source": [
    "pre_trans = weights.transforms()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to randomly augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [TorchVision Transforms class](https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "**Hint**: Remember not to make the data augmentation too extreme."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:19.943910Z",
     "start_time": "2025-08-09T04:54:19.935998Z"
    }
   },
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomResizedCrop((IMG_HEIGHT, IMG_WIDTH), scale=(0.8, 1.0)),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:21.552922Z",
     "start_time": "2025-08-09T04:54:21.544544Z"
    }
   },
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"] \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the batch size `n` and set `shuffle` either to `True` or `False` depending on if we are `train`ing or `valid`ating. For a reference, check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T04:54:34.945686Z",
     "start_time": "2025-08-09T04:54:25.423554Z"
    }
   },
   "source": [
    "n = 32\n",
    "\n",
    "train_path = \"data/fruits/train/\"\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_path = \"data/fruits/valid/\"\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! We've moved the `train` and `validate` functions to our [utils.py](./utils.py) file. Before running the below, make sure all your variables are correctly defined.\n",
    "\n",
    "It may help to rerun this cell or change the number of `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:39:03.801710Z",
     "start_time": "2025-08-09T04:54:34.965544Z"
    }
   },
   "source": [
    "epochs = 10\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] WON'T CONVERT inner D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\external_utils.py line 68 \n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] due to: \n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Traceback (most recent call last):\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1213, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     result = self._inner_convert(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 598, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1059, in _compile\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 97, in wrapper_function\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return function(*args, **kwargs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 761, in compile_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 797, in _compile_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     out_code = transform_code_object(code, transform)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1422, in transform_code_object\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     transformations(instructions, code_options)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 257, in _fn\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return fn(*args, **kwargs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 715, in transform\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     tracer.run()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3498, in run\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     super().run()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1337, in run\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     while self.step():\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1246, in step\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3699, in RETURN_VALUE\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self._return(inst)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3684, in _return\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.output.compile_subgraph(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1144, in compile_subgraph\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.compile_and_call_fx_graph(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1437, in compile_and_call_fx_graph\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1487, in call_user_compiler\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._call_user_compiler(gm)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1519, in _call_user_compiler\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 150, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2347, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 2100, in compile_fx\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 760, in _compile_fx_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise InductorError(e, currentframe()).with_traceback(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 745, in _compile_fx_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1295, in fx_codegen_and_compile\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1197, in codegen_and_compile\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = graph.compile_to_module().call\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2083, in compile_to_module\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._compile_to_module()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2091, in _compile_to_module\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                              ^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2002, in codegen\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.scheduler.codegen()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4135, in codegen\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     else self._codegen(self.nodes)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4264, in _codegen\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.get_backend(device).codegen_node(node)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4986, in codegen_node\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3734, in __init__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 418, in pick_vec_isa\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 405, in valid_vec_isa_list\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     isa_list.extend(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 408, in <genexpr>\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                                             ^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 142, in __bool__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.__bool__impl(config.cpp.vec_isa_ok)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 152, in __bool__impl\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.check_build(VecISA._avx_code)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 102, in check_build\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 28, in _get_isa_dry_compile_fingerprint\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 148, in get_cpp_compiler\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     check_compiler_exist_windows(compiler)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 139, in check_compiler_exist_windows\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Traceback (most recent call last):\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1213, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     result = self._inner_convert(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 598, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1059, in _compile\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 97, in wrapper_function\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return function(*args, **kwargs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 761, in compile_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 797, in _compile_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     out_code = transform_code_object(code, transform)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1422, in transform_code_object\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     transformations(instructions, code_options)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 257, in _fn\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return fn(*args, **kwargs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 715, in transform\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     tracer.run()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3498, in run\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     super().run()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1337, in run\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     while self.step():\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1246, in step\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3699, in RETURN_VALUE\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self._return(inst)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3684, in _return\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.output.compile_subgraph(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1144, in compile_subgraph\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.compile_and_call_fx_graph(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1437, in compile_and_call_fx_graph\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1487, in call_user_compiler\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._call_user_compiler(gm)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1519, in _call_user_compiler\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 150, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2347, in __call__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 2100, in compile_fx\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 760, in _compile_fx_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise InductorError(e, currentframe()).with_traceback(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 745, in _compile_fx_inner\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1295, in fx_codegen_and_compile\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1197, in codegen_and_compile\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = graph.compile_to_module().call\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2083, in compile_to_module\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._compile_to_module()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2091, in _compile_to_module\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                              ^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2002, in codegen\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.scheduler.codegen()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4135, in codegen\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     else self._codegen(self.nodes)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4264, in _codegen\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.get_backend(device).codegen_node(node)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4986, in codegen_node\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3734, in __init__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 418, in pick_vec_isa\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 405, in valid_vec_isa_list\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     isa_list.extend(\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 408, in <genexpr>\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                                             ^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 142, in __bool__\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.__bool__impl(config.cpp.vec_isa_ok)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 152, in __bool__impl\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.check_build(VecISA._avx_code)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 102, in check_build\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 28, in _get_isa_dry_compile_fingerprint\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 148, in get_cpp_compiler\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     check_compiler_exist_windows(compiler)\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 139, in check_compiler_exist_windows\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "W0809 11:54:39.840000 5844 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 22.5321 Accuracy: 0.7724\n",
      "Valid - Loss: 12.5963 Accuracy: 0.7143\n",
      "Epoch: 1\n",
      "Train - Loss: 13.4207 Accuracy: 0.8596\n",
      "Valid - Loss: 5.5847 Accuracy: 0.8815\n",
      "Epoch: 2\n",
      "Train - Loss: 8.9247 Accuracy: 0.9086\n",
      "Valid - Loss: 3.4911 Accuracy: 0.8936\n",
      "Epoch: 3\n",
      "Train - Loss: 8.8269 Accuracy: 0.9095\n",
      "Valid - Loss: 2.4725 Accuracy: 0.9210\n",
      "Epoch: 4\n",
      "Train - Loss: 5.8610 Accuracy: 0.9365\n",
      "Valid - Loss: 1.9060 Accuracy: 0.9240\n",
      "Epoch: 5\n",
      "Train - Loss: 7.2149 Accuracy: 0.9289\n",
      "Valid - Loss: 4.2733 Accuracy: 0.8936\n",
      "Epoch: 6\n",
      "Train - Loss: 6.4390 Accuracy: 0.9416\n",
      "Valid - Loss: 2.6522 Accuracy: 0.9210\n",
      "Epoch: 7\n",
      "Train - Loss: 6.8802 Accuracy: 0.9349\n",
      "Valid - Loss: 2.0868 Accuracy: 0.9240\n",
      "Epoch: 8\n",
      "Train - Loss: 6.4756 Accuracy: 0.9391\n",
      "Valid - Loss: 3.2536 Accuracy: 0.9088\n",
      "Epoch: 9\n",
      "Train - Loss: 5.7854 Accuracy: 0.9450\n",
      "Valid - Loss: 3.2643 Accuracy: 0.9149\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:40:17.448477Z",
     "start_time": "2025-08-09T05:40:17.429573Z"
    }
   },
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(False)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:45:19.188044Z",
     "start_time": "2025-08-09T05:40:19.268441Z"
    }
   },
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 4.3118 Accuracy: 0.9552\n",
      "Valid - Loss: 3.4325 Accuracy: 0.9149\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation. \n",
    "\n",
    "Once you are satisfied with the validation accuracy, evaluate the model by executing the following cell. The evaluate function will return a tuple, where the first value is your loss, and the second value is your accuracy. To pass, the model will need have an accuracy value of `92% or higher`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess your model run the following two cells.\n",
    "\n",
    "**NOTE:** `run_assessment` assumes your model is named `my_model`. If for any reason you have modified these variable names, please update the names of the arguments passed to `run_assessment`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T05:59:09.156755Z",
     "start_time": "2025-08-09T05:59:08.826663Z"
    }
   },
   "source": [
    "from run_assessment import run_assessment"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'run_assessment'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrun_assessment\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m run_assessment\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'run_assessment'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_assessment(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
