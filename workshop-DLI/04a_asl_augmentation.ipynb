{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1AHrcF83Y-g"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBNeKAyF3Y-h"
   },
   "source": [
    "# 4a. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTHY1Otu3Y-h"
   },
   "source": [
    "So far, we've selected a model architecture that vastly improves the model's performance, as it is designed to recognize important features in the images. The validation accuracy is still lagging behind the training accuracy, which is a sign of overfitting: the model is getting confused by things it has not seen before when it tests against the validation dataset.\n",
    "\n",
    "In order to teach our model to be more robust when looking at new data, we're going to programmatically increase the size and variance in our dataset. This is known as [*data augmentation*](https://link.springer.com/article/10.1186/s40537-019-0197-0), a useful technique for many deep learning applications.\n",
    "\n",
    "The increase in size gives the model more images to learn from while training. The increase in variance helps the model ignore unimportant features and select only the features that are truly important in classification, allowing it to generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k01AskqI3Y-h"
   },
   "source": [
    "## 4a.1 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCFOyxKS3Y-h"
   },
   "source": [
    "* Augment the ASL dataset\n",
    "* Use the augmented data to train an improved model\n",
    "* Save the well-trained model to disk for use in deployment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6560,
     "status": "ok",
     "timestamp": 1715241340700,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "ocl26UO63Y-i",
    "outputId": "b097ecfc-e330-4c6e-d386-4b2b7cbb55bb",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:12:24.412224Z",
     "start_time": "2025-08-09T02:12:17.341206Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-FCWlRg3Y-h"
   },
   "source": [
    "## 4a.2 Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjSagpmG3Y-i"
   },
   "source": [
    "As we're in a new notebook, we will load and process our data again. To do this, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 3988,
     "status": "ok",
     "timestamp": 1715241345056,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "jYhhD7yo2WEI",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:12:36.849127Z",
     "start_time": "2025-08-09T02:12:33.422445Z"
    }
   },
   "source": [
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "IMG_CHS = 1\n",
    "N_CLASSES = 24\n",
    "\n",
    "train_df = pd.read_csv(\"data/asl_data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"data/asl_data/sign_mnist_valid.csv\")\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, base_df):\n",
    "        x_df = base_df.copy()\n",
    "        y_df = x_df.pop('label')\n",
    "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
    "        x_df = x_df.reshape(-1, IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
    "        self.xs = torch.tensor(x_df).float().to(device)\n",
    "        self.ys = torch.tensor(y_df).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "n = 32\n",
    "train_data = MyDataset(train_df)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_data = MyDataset(valid_df)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n)\n",
    "valid_N = len(valid_loader.dataset)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwsfoZkE3Y-i"
   },
   "source": [
    "## 4a.3 Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze7Tv-Aj3Y-i"
   },
   "source": [
    "We will also need to create our model again. As we learned in the last lesson, convolutional neural networks use a repeated sequence of layers. Let's take advantage of this pattern to make our own [custom module](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html). We can then use this module like a layer in our [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model.\n",
    "\n",
    "To do this, we will extend the [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class. Then we will define two methods:\n",
    "* `__init__`: defines any properties we want our module to have, including our neural network layers. We will effectively be using a model within a model.\n",
    "* `forward`: defines how we want the module to process any incoming data from the previous layer it is connected to. Since we are using a `Sequential` model, we can pass the input data into it like we are making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1715241347583,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "_o8Y7C91Bfl8",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:12:43.434039Z",
     "start_time": "2025-08-09T02:12:43.425380Z"
    }
   },
   "source": [
    "class MyConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout_p):\n",
    "        kernel_size = 3\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've define our custom module, let's see it in action. The below model ia archecturially the same as in the previous lesson. Can you see the connection?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715241351435,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "I0A_7iJvB8Kc",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:12:50.240573Z",
     "start_time": "2025-08-09T02:12:50.224091Z"
    }
   },
   "source": [
    "flattened_img_size = 75 * 3 * 3\n",
    "\n",
    "# Input 1 x 28 x 28\n",
    "base_model = nn.Sequential(\n",
    "    MyConvBlock(IMG_CHS, 25, 0), # 25 x 14 x 14\n",
    "    MyConvBlock(25, 50, 0.2), # 50 x 7 x 7\n",
    "    MyConvBlock(50, 75, 0),  # 75 x 3 x 3\n",
    "    # Flatten to Dense Layers\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(flattened_img_size, 512),\n",
    "    nn.Dropout(.3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, N_CLASSES)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model, not only will it now show the use of our custom module, it will also show the layers within our custom module:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1715241354080,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "4THc2t0HhNcv",
    "outputId": "e25d69a9-e51a-4a90-90df-dc69a586f54b",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:12:53.710212Z",
     "start_time": "2025-08-09T02:12:52.461844Z"
    }
   },
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(base_model.parameters())\n",
    "\n",
    "model = torch.compile(base_model.to(device))\n",
    "model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Sequential(\n",
       "    (0): MyConvBlock(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0, inplace=False)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MyConvBlock(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(25, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MyConvBlock(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(50, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0, inplace=False)\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=675, out_features=512, bias=True)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): ReLU()\n",
       "    (7): Linear(in_features=512, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom modules are flexible, and we can define any other methods or properties we wish to have. This makes them powerful when data scientists are trying to solve complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjBNCzfc3Y-j"
   },
   "source": [
    "## 4a.4 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8HdHKtM3Y-j"
   },
   "source": [
    "Before defining our training loop, it's time to set up our data augmentation.\n",
    "\n",
    "We've seen [TorchVision](https://pytorch.org/vision/stable/index.html)'s [Transforms](https://pytorch.org/vision/0.9/transforms.html) before, but in this lesson, we will further explore its data augmentation tools. First, let's get a sample image to test with:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1715241358482,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "-LT7NvrXhYwB",
    "outputId": "4c1c1af4-811b-46d7-fa73-594772907549",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:13:35.908966Z",
     "start_time": "2025-08-09T02:13:35.879116Z"
    }
   },
   "source": [
    "row_0 = train_df.head(1)\n",
    "y_0 = row_0.pop('label')\n",
    "x_0 = row_0.values / 255\n",
    "x_0 = x_0.reshape(IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
    "x_0 = torch.tensor(x_0)\n",
    "x_0.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1715241364072,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "XKFRYIpvkUEF",
    "outputId": "fb3f72ab-ce59-4bfc-a54a-0a4d575e497c",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:14:20.752436Z",
     "start_time": "2025-08-09T02:14:19.513583Z"
    }
   },
   "source": [
    "image = F.to_pil_image(x_0)\n",
    "plt.imshow(image, cmap='gray')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec1d943e90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.4.1 [RandomResizeCrop](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomResizedCrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transform will randomly resize the input image based on `scale`, and then [crop](https://en.wikipedia.org/wiki/Cropping_(image)) it to a size we specify. In this case, we will crop it to the original image dimensions. To do this, TorchVision needs to know the [aspect ratio](https://en.wikipedia.org/wiki/Aspect_ratio_(image)) of the image it is scaling. Since our height is the same as our width, our aspect `ratio` is 1:1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715241375000,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "qWINTqKypE5J",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:15:30.295337Z",
     "start_time": "2025-08-09T02:15:30.278593Z"
    }
   },
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.7, 1), ratio=(1, 1)),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the below cell a few times. It should be different each time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1715241377237,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "6ZugUNuJpPG2",
    "outputId": "52caec17-6a25-4484-c2f4-2aed78b5ffe8",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:18:03.494976Z",
     "start_time": "2025-08-09T02:18:03.238253Z"
    }
   },
   "source": [
    "new_x_0 = trans(x_0)\n",
    "image = F.to_pil_image(new_x_0)\n",
    "plt.imshow(image, cmap='gray')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec432ccaa0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAH4RJREFUeJzt3X1sVfUdx/EvtY+UPtACfZCWB/FhyoPRIWOg4mBUlhhRssg0CywGAgMz7Jymi4q4Jd00cUbD4J9NZqKgJoLRLBhFKXGjbtYRQtSOkk4gUKDMPtIHaM/yO6YdlQf7+7X9fU/vfb+Sk3Lb++Oce+6593PPPed+7oggCAIBAMCzBN8zBADAIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgIlEipru7W44dOyYZGRkyYsQI7cUBAFgy/QbNzc1SWFgoCQkJwyeATPgUFRVpLwYAYICOHDki48ePHz4BZPZ8jKVLl0pycnK/x2VnZzvPy1Z6err1mJSUFOsxNre/R1JSkpcxruMSE+03uSuuuMLLGJ/L197ebj1m5MiR3taD6zhbl3t1jG9/t8gX28a21tZWueeee771OXbIAmjjxo3y7LPPSl1dncyYMUNefPFFueWWW751XM/bbubJ1+YJ2OUJPjU11XqM6ziXMb4CyGU+rvPy9QTvMh+fy+fyxOvywocAGhiXwwC+6jW7IxxA/V1/Q3Lvv/baa1JaWirr16+XTz/9NAygkpISOXny5FDMDgAwDA1JAD333HOyYsUK+dnPfibXX3+9bN68OXz74M9//vNQzA4AMAwNegB1dnZKVVWVLFiw4P8zSUgIL+/du/eC63d0dEhTU1OfCQAQ+wY9gOrr66Wrq0vy8vL6/N5cNseDvqm8vFyysrJ6J86AA4D4oH4EsKysTBobG3snc9oeACD2DfpZcGPGjAnPoDlx4kSf35vL+fn5Fz17zeUMNgDA8Dboe0DmlN6bb75Zdu3a1ed0QXN59uzZgz07AMAwNSSfAzKnYC9btky++93vhp/9ef7558MPJpmz4gAAGLIAuu++++TUqVPy5JNPhice3HjjjbJz584LTkwAAMSvIWtCWLt2bTi5MseFbD6h7+sT7K6f3vb1aXlfY1w/Je4yL5f71rUJwWX5XFou3n33Xesx5q1tW8XFxeKLr0/mR72k2NfyJXhskbC9b/u7DtTPggMAxCcCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAACxVUY6GEV7NgWePgsrfRWLupQautwm1/JEX7cp6uWTLtvDyZMnrceYbwz2VRDqs9Q2ynzdpm5PRa6+1kN/rx97WwwAYFgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKhIjHLDsE3LsEsjscsY13Eujc6+GoldG3+jfJtcubSJt7e3W49paGiIbAv7cGggjzUJDveTzwbtIAiG5P9lDwgAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAICKmCkjdSnzcy1cdJmXS5FkUlKSl/m4FHC6zivKRa5GRkaG9ZjDhw9bj2lpabEek5WVFeltPBZFuZQ1wfE+cikxHar1wFYGAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABARcyUkfoqxnQd57Ms1RdfpawuY4IgEBeZmZnWY5qbm73cpuzsbPEl6ttelMtVXco+ExyWz2U+rvMaqnmwBwQAUEEAAQBiI4CeeuqpcPf9/Om6664b7NkAAIa5ITkGdMMNN8j7778/4C88AwDEriFJBhM4+fn5Q/FfAwBixJAcAzp48KAUFhbK5MmT5YEHHrjsVxZ3dHRIU1NTnwkAEPsGPYBmzZolW7ZskZ07d8qmTZuktrZWbr311kueqlpeXh5+133PVFRUNNiLBACIhwBatGiR/PjHP5bp06dLSUmJ/PWvf5WGhgZ5/fXXL3r9srIyaWxs7J2OHDky2IsEAIigIT87wHx47pprrpGampqL/j0lJSWcAADxZcg/B9TS0iKHDh2SgoKCoZ4VACCeA+iRRx6RiooK+c9//iN///vf5Z577glrR37yk58M9qwAAMPYoL8Fd/To0TBsTp8+LWPHjpW5c+dKZWVl+G8AAIYsgLZt2zYo/09SUlI4Ra0g1LWo0Ve5o8t8XEtZbe4f3/dTWlqauDh79qz1mMt9zOBSUlNTvdwm123cVzmmr5Jen2Wkro+nKLO9b/t7H9EFBwBQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAIDa/kC7KBYCupYEu4xITEyN7m1yWzWfBqovMzEyncU1NTdZjqqurrccUFxd7KTB1vY9ctwlEX1dXl0QFe0AAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABWRrbw1rc42jbxRb8N2aSVOSEjwMsa1MdllXi7rzmU+SUlJ4mLkyJHWY3Jzc63HjB492npMQ0ODl/m4cn08+dhegyCQWGuoTnB4XPha5/3dFtgDAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoCKyZaSmTNKmUNJXcadrGaKvAlNfpac+5+Wy7myKbAc6r9tvv916TGpqqvWYiooK6zFFRUXiYtasWV4KNV243LeuRand3d1eik8THW6Tz4JV28dtf28Pe0AAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBURLaM1BRd2pRd+ioIdS0O9FWW6jLGZd25zstXGamr5ORk6zHnzp2zHvPKK69Yj8nLy7MeU1VVJS5uuukm6zFtbW3WY+rq6qzHTJ061ds27rLt+SoJ7XYoSnV9/nLZxvuDPSAAgAoCCAAwPAJoz549ctddd0lhYWG4W7tjx44Ldj+ffPJJKSgokLS0NFmwYIEcPHhwMJcZABCPAdTa2iozZsyQjRs3XvTvzzzzjLzwwguyefNm+fjjjyU9PV1KSkqkvb19MJYXABAjrI9GLVq0KJwuxuz9PP/88/L444/L3XffHf7u5ZdfDg+emj2lpUuXDnyJAQAxYVCPAdXW1oZntZi33XpkZWWFX++7d+/ei47p6OiQpqamPhMAIPYNagD1nFL5zdNFzeVLnW5ZXl4ehlTP5Pod9gCA4UX9LLiysjJpbGzsnY4cOaK9SACA4RZA+fn54c8TJ070+b253PO3b0pJSZHMzMw+EwAg9g1qAE2aNCkMml27dvX+zhzTMWfDzZ49ezBnBQCIt7PgWlpapKamps+JB/v27ZOcnBwpLi6WdevWyW9/+1u5+uqrw0B64oknws8MLV68eLCXHQAQTwH0ySefyB133NF7ubS0NPy5bNky2bJlizz66KPhZ4VWrlwpDQ0NMnfuXNm5c6ekpqYO7pIDAOIrgObNm3fZsj3TjvD000+H04AWLDHRqjTPZwmnr1LDKI9xHWeO+dkyrRq+ihr/+9//Wo8xL7hsNTc3W49x+TC3y7ozXNpLRo4caT2murraeoz5IHys6XbYXl0fty5lqbbz6u/11c+CAwDEJwIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIADA8GjDjiqXZliXBu2BjIu1Nuz09HQvY3w1VBunT5+2HvPpp59aj8nIyLAec+rUKesxU6ZMERd1dXXiQ1VVlfWY+fPnW48ZN26cxFqzdeDQam10dXUN+TcH9Pf67AEBAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQETNlpC4FobYFez0SExMjW5bqMiYnJ0dcjBo1ynpMamqq9ZiOjg5vBasnT570MsalYLWzs9NLuarR1NRkPSYvL896zJkzZ6zH7Nixw3rMypUrxYXLc4SvYtFuh9LTgTw2huJ5iD0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKiJbRmoK82xK81xKA11KRX0Xn9rKysqyHnPVVVc5zcul6PLzzz+3HnPq1CnrMc3NzeIiKSnJeszEiROtx9TX13sp7qytrRUXX331lfWYm266yXpMRkaG9Zh///vf1mPa29vFRXp6ureSUB/PQ67LZ/v81d/rswcEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABARWTLSH1wLQj1VXzqMp+UlBTrMZ2dneLixIkT1mP2799vPaaystJ6zGeffSYuXIpZb731Vusx2dnZXtZ3R0eHuHAp73Qp7jx79qyXwti0tDRx4atEeITDfIIg8FpiOhTzYA8IAKCCAAIADI8A2rNnj9x1111SWFgY7jbu2LGjz9+XL18e/v786c477xzMZQYAxGMAtba2yowZM2Tjxo2XvI4JnOPHj/dOW7duHehyAgBijPWR8UWLFoXTtx0Iz8/PH8hyAQBi3JAcA9q9e7eMGzdOrr32Wlm9evVlv7rZnKXT1NTUZwIAxL5BDyDz9tvLL78su3btkt///vdSUVER7jF1dXVd9Prl5eWSlZXVOxUVFQ32IgEA4uFzQEuXLu3997Rp02T69OnhZyvMXtH8+fMvuH5ZWZmUlpb2XjZ7QIQQAMS+IT8Ne/LkyTJmzBipqam55PGizMzMPhMAIPYNeQAdPXo0PAZUUFAw1LMCAMTyW3AtLS199mZqa2tl3759kpOTE04bNmyQJUuWhGfBHTp0SB599FGZMmWKlJSUDPayAwDiKYA++eQTueOOO3ov9xy/WbZsmWzatCns+vrLX/4iDQ0N4YdVFy5cKL/5zW+cOsoAALHLOoDmzZt32RK8d999VwZDT4tCf11xxRWRLOUbSNmgy21y4VII2fP2qq3q6mrrMV988YWX0lPjyy+/tB7zwx/+0Ms6dykIzcjIEBcuxaIuhbsuj8GJEyd6eyz5uk1dlzhLeCh0d3cP+W2ijBQAEGkEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAgNj4Su7BYtpUbRpYXRpoXRqqXZt1fTVbuzTdXq7d/HJSU1Otx+Tm5lqPGT16tLf71uXr4F1aquvr663HNDY2Wo9JTk4WF1deeaX1mLa2Nusxzc3N1mOmTp1qPcbn18G4bntRXj7b54j+Pt+xBwQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEBFYpQL86Je6jfUhZ8ut9+ljNRljJGUlGQ9Ji0tzXrM2LFjrccUFxeLi6+++sp6zD//+U/rMQ0NDV6KUidMmCC+imZbW1utx5w9e9Z6TGFhobcyYJfHhq+y4sCxRLirq2vI1wNlpACASCOAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKAi0mWkCQn9z0efxaU2yzWQskGX25SYmOit1NBFenq69Zj8/HxvJZw1NTXWY+rr663HXH/99V5KWTMzM8XF6dOnvRS5utymgoIC6zGuzw8uj3UX3Y6FwC58PBdRRgoAiDQCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqIltGasrsbErzfBWEus7Ll7Nnz1qPaW9v91agmJyc7GU+rvftqFGjvJSlXn311V6KXFtbW8XXduSyfKtXr/YyH9fHrMu25zImweNzio/l6+/1o/tMCgCIaQQQACD6AVReXi4zZ86UjIwMGTdunCxevFiqq6sveDtnzZo1kpubG76dsWTJEjlx4sRgLzcAIJ4CqKKiIgyXyspKee+998L3iRcuXNjnfeaHH35Y3n77bXnjjTfC6x87dkzuvffeoVh2AEC8nISwc+fOPpe3bNkS7glVVVXJbbfdJo2NjfKnP/1JXn31VfnBD34QXuell16S73znO2Fofe973xvcpQcAxOcxIBM4Rk5OTvjTBJHZK1qwYEHvda677jopLi6WvXv3XvT/6OjokKampj4TACD2JQzkVL5169bJnDlzZOrUqeHv6urqwtNss7Oz+1w3Ly8v/NuljitlZWX1TkVFRa6LBACIhwAyx4IOHDgg27ZtG9AClJWVhXtSPdORI0cG9P8BAGL4g6hr166Vd955R/bs2SPjx4/v84G8zs5OaWho6LMXZM6Cu9SH9VJSUsIJABBfrPaAgiAIw2f79u3ywQcfyKRJk/r8/eabb5akpCTZtWtX7+/MadqHDx+W2bNnD95SAwDiaw/IvO1mznB76623ws8C9RzXMcdu0tLSwp8PPviglJaWhicmZGZmykMPPRSGD2fAAQCcA2jTpk3hz3nz5vX5vTnVevny5eG///CHP4Q9QOYDqOYMt5KSEvnjH/9oMxsAQBxItH0L7tukpqbKxo0bw2kgurq6wqm/RowY4a0A0GVeLmNcCjVdSiR9srlPB1KWmpjo1rNr9tp9FKyaY6U+SiRbWlrEhcvHIX76059aj5kyZYr4EPWyzxEOzw/9eT4erHVhe5v6e3voggMAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqHCrDPbANEHbtEH7aqh2bal2aaD11eDb1tbmNM6lpdplXi7rITc3V1y4NAy7bA/mq0pstba2Wo85ffq0uBg9erT1mFmzZlmPMV9g6aNt2rU52uW+dRnT7XCbXJrlXR9P586dG5J5sAcEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABARWTLSE2ZnU1pnq+CUNdxvgoKU1NTvZUa2hYUusrKyrIec/bsWad5uRTUNjc3e1nnZ86csR7T2dkpLpYtW+alwNSFr5JeVy6P24SI36bExMQhuX60bzUAIGYRQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQEdkyUh8lkj7n5VJQmJSUZD0mOTnZekxbW5u4cCn8dFkPLqWnra2t4qKjo8N6TF1dnZfSWJdS1jVr1oiL73//+14eFy4lnC7bkKsgCCJ7m65wKDh2vU1DtWzsAQEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFAR2TJSU2bnWrZnMw9fXIpFJ02aZD3m6NGj1mOOHz8uLlpaWqzHfPXVV15uU319vbjo7OwUH9LT063HPPzww9ZjZsyYIVEu9/VVYOqTS7Fogsfb5LJ8Q1VgGu17EgAQswggAED0A6i8vFxmzpwpGRkZMm7cOFm8eLFUV1f3uc68efPC3erzp1WrVg32cgMA4imAKioqwi+4qqyslPfeey/8QrKFCxde8OVfK1asCI8r9EzPPPPMYC83ACCeTkLYuXNnn8tbtmwJ94Sqqqrktttu6/39yJEjJT8/f/CWEgAQcwZ0DKixsTH8mZOT0+f3r7zyiowZM0amTp0qZWVlcubMmct+BXJTU1OfCQAQ+xIHcirfunXrZM6cOWHQ9Lj//vtlwoQJUlhYKPv375fHHnssPE705ptvXvK40oYNG1wXAwAQbwFkjgUdOHBAPvrooz6/X7lyZe+/p02bJgUFBTJ//nw5dOiQXHXVVRf8P2YPqbS0tPey2QMqKipyXSwAQCwH0Nq1a+Wdd96RPXv2yPjx4y973VmzZoU/a2pqLhpAKSkp4QQAiC+Jtp+Gfeihh2T79u2ye/fufn1Sf9++feFPsycEAIBTAJm33V599VV56623ws8C1dXVhb/PysqStLS08G028/cf/ehHkpubGx4DMvUh5gy56dOn28wKABDjrAJo06ZNvR82Pd9LL70ky5cvl+TkZHn//ffl+eefDz8bZI7lLFmyRB5//PHBXWoAQPy9BXc5JnDMh1UBABi2bdg+2mR9NtB+28kag9VaO3bsWOsx5gQRF9+sYeqPhoYGL83WbW1t4otLU3BJSYn1mBtvvNFb47uvx4av1u2hanPWXA+B423yMa/+bneUkQIAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFARM2WkPqWnp1uPGTVqlPWYlpYWLyWS5vuaXJw4ccJL8alLoaZLkasrl3U+d+5cL+vBtYw01riWnroUfvpa511dXTLcsQcEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABWR64Lr6V5qa2uzGpeYmOitsyk5Odl6TFNTk/WY1tZW8cF1PbS3t1uPOXv2rPWYc+fORbony6V3zqXnz2UbogtuYFy64HzpctzGXbZX2/XQ3Nzcr3Ejgoit4aNHj0pRUZH2YgAABujIkSMyfvz44RNAJp2PHTsmGRkZFzTYmleAJpzMjcrMzJR4xXr4Guvha6yHr7EeorMeTKyYvaDCwsLLtsVH7i04s7CXS0zDrNR43sB6sB6+xnr4Guvha6yHaKyHrKysb70OJyEAAFQQQAAAFcMqgFJSUmT9+vXhz3jGevga6+FrrIevsR6G33qI3EkIAID4MKz2gAAAsYMAAgCoIIAAACoIIACAimETQBs3bpSJEydKamqqzJo1S/7xj39IvHnqqafCdojzp+uuu05i3Z49e+Suu+4KP1VtbvOOHTv6/N2cR/Pkk09KQUGBpKWlyYIFC+TgwYMSb+th+fLlF2wfd955p8SS8vJymTlzZtiUMm7cOFm8eLFUV1df0FG4Zs0ayc3NlVGjRsmSJUvkxIkTEm/rYd68eRdsD6tWrZIoGRYB9Nprr0lpaWl4auGnn34qM2bMkJKSEjl58qTEmxtuuEGOHz/eO3300UcS60wpq7nPzYuQi3nmmWfkhRdekM2bN8vHH38s6enp4fbhUpY6nNeDYQLn/O1j69atEksqKirCcKmsrJT33nsvLLdduHBhn+Lehx9+WN5++2154403wuubaq97771X4m09GCtWrOizPZjHSqQEw8Att9wSrFmzpvdyV1dXUFhYGJSXlwfxZP369cGMGTOCeGY22e3bt/de7u7uDvLz84Nnn32293cNDQ1BSkpKsHXr1iBe1oOxbNmy4O677w7iycmTJ8N1UVFR0XvfJyUlBW+88UbvdT7//PPwOnv37g3iZT0Yt99+e/CLX/wiiLLI7wF1dnZKVVVV+LbK+X1x5vLevXsl3pi3lsxbMJMnT5YHHnhADh8+LPGstrZW6urq+mwfpoPKvE0bj9vH7t27w7dkrr32Wlm9erWcPn1aYlljY2P4MycnJ/xpnivM3sD524N5m7q4uDimt4fGb6yHHq+88oqMGTNGpk6dKmVlZXLmzBmJksiVkX5TfX19+L0XeXl5fX5vLn/xxRcST8yT6pYtW8InF7M7vWHDBrn11lvlwIED4XvB8ciEj3Gx7aPnb/HCvP1m3mqaNGmSHDp0SH7961/LokWLwifeWPxeINOcv27dOpkzZ074BGuY+9x8X1d2dnbcbA/dF1kPxv333y8TJkwIX7Du379fHnvssfA40ZtvvilREfkAwv+ZJ5Me06dPDwPJbGCvv/66PPjgg6rLBn1Lly7t/fe0adPCbeSqq64K94rmz58vscYcAzEvvuLhOKjLeli5cmWf7cGcpGO2A/PixGwXURD5t+DM7qN59fbNs1jM5fz8fIln5lXeNddcIzU1NRKverYBto8LmbdpzeMnFrePtWvXyjvvvCMffvhhn69vMfe5edu+oaEhLraHtZdYDxdjXrAaUdoeIh9AZnf65ptvll27dvXZ5TSXZ8+eLfHMfK2zeTVjXtnEK/N2k3liOX/7MF/IZc6Gi/ftw3y7sDkGFEvbhzn/wjzpbt++XT744IPw/j+fea5ISkrqsz2Yt53MsdJY2h6Cb1kPF7Nv377wZ6S2h2AY2LZtW3hW05YtW4LPPvssWLlyZZCdnR3U1dUF8eSXv/xlsHv37qC2tjb429/+FixYsCAYM2ZMeAZMLGtubg7+9a9/hZPZZJ977rnw319++WX499/97nfh9vDWW28F+/fvD88EmzRpUtDW1hbEy3owf3vkkUfCM73M9vH+++8HN910U3D11VcH7e3tQaxYvXp1kJWVFT4Ojh8/3judOXOm9zqrVq0KiouLgw8++CD45JNPgtmzZ4dTLFn9LeuhpqYmePrpp8Pbb7YH89iYPHlycNtttwVRMiwCyHjxxRfDjSo5OTk8LbuysjKIN/fdd19QUFAQroMrr7wyvGw2tFj34Ycfhk+435zMacc9p2I/8cQTQV5eXvhCZf78+UF1dXUQT+vBPPEsXLgwGDt2bHga8oQJE4IVK1bE3Iu0i91+M7300ku91zEvPH7+858Ho0ePDkaOHBncc8894ZNzPK2Hw4cPh2GTk5MTPiamTJkS/OpXvwoaGxuDKOHrGAAAKiJ/DAgAEJsIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCIhv8BuVhppbgs918AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1715241385987,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "8VQJ1vwKp4nJ",
    "outputId": "63521e3a-5a63-48c8-8823-bd60d6814b64",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:18:33.354730Z",
     "start_time": "2025-08-09T02:18:33.348832Z"
    }
   },
   "source": [
    "new_x_0.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.4.2 [RandomHorizontalFlip](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomHorizontalFlip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yrmm_inJ3Y-j"
   },
   "source": [
    "We can also randomly flip our images [Horizontally](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomHorizontalFlip) or [Vertically](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomVerticalFlip). However, for these images, we will only flip them horizontally.\n",
    "\n",
    "Take a moment to think about why we would want to flip images horizontally, but not vertically. When you have an idea, reveal the text below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCLufCeF3Y-j"
   },
   "source": [
    "`# SOLUTION` Fun fact: American Sign Language can be done with either the left or right hand being dominant. However, it is unlikely to see sign language from upside down. This kind of domain-specific reasoning can help make good decisions for your own deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:19:54.631250Z",
     "start_time": "2025-08-09T02:19:54.625373Z"
    }
   },
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the below cell a few times. Does the image flip about half the time?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:19:56.736152Z",
     "start_time": "2025-08-09T02:19:56.608006Z"
    }
   },
   "source": [
    "new_x_0 = trans(x_0)\n",
    "image = F.to_pil_image(new_x_0)\n",
    "plt.imshow(image, cmap='gray')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec433736b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.4.3 [RandomRotation](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.RandomRotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also randomly rotate the image to add more variability. Just like with with other augmentation techniques, it's easy to accidentally go too far. With ASL, if we rotate too much, our `D`s might look like `G`s and visa versa. Because of this, let's limit it to `30` degrees."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:20:35.488916Z",
     "start_time": "2025-08-09T02:20:35.484021Z"
    }
   },
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.RandomRotation(10)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the cell block below, some black pixels may appear. The corners or our image disappear when we rotate, and for almost every pixel we lose, we gain an empty pixel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:20:52.429107Z",
     "start_time": "2025-08-09T02:20:52.304254Z"
    }
   },
   "source": [
    "new_x_0 = trans(x_0)\n",
    "image = F.to_pil_image(new_x_0)\n",
    "plt.imshow(image, cmap='gray')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec43930b00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.4.3 [ColorJitter](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.ColorJitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ColorJitter` transform has 4 arguments:\n",
    "* [brightness](https://en.wikipedia.org/wiki/Brightness)\n",
    "* [contrast](https://en.wikipedia.org/wiki/Contrast_(vision))\n",
    "* [saturation](https://en.wikipedia.org/wiki/Colorfulness#Saturation)\n",
    "* [hue](https://en.wikipedia.org/wiki/Hue)\n",
    "\n",
    "\n",
    "The latter 2 apply to color images, so we will only use the first 2 for now."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:22:16.945262Z",
     "start_time": "2025-08-09T02:22:16.938327Z"
    }
   },
   "source": [
    "brightness = 1  # Change to be from 0 to 1\n",
    "contrast = 0  # Change to be from 0 to 1\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=brightness, contrast=contrast)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the below a few times, but also try changing either `brightness` or `contrast` to `1`. Get any intersting results?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:22:20.940631Z",
     "start_time": "2025-08-09T02:22:20.811326Z"
    }
   },
   "source": [
    "new_x_0 = trans(x_0)\n",
    "image = F.to_pil_image(new_x_0)\n",
    "plt.imshow(image, cmap='gray')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec43afef60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.3.4 [Compose](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.Compose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to bring it all together. We can create a sequence of these random transformations with `Compose`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1715241387886,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "ZkXjesFKFH_b",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:23:16.675929Z",
     "start_time": "2025-08-09T02:23:16.669803Z"
    }
   },
   "source": [
    "random_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.9, 1), ratio=(1, 1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=.2, contrast=.5)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out. With all the different combinations how many varations are there of this one image? Infinite?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1715241391170,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "ewG_7NAgqEnf",
    "outputId": "24142f9f-286f-42ab-9769-bfd38c9defbf",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:23:28.384673Z",
     "start_time": "2025-08-09T02:23:28.110829Z"
    }
   },
   "source": [
    "new_x_0 = random_transforms(x_0)\n",
    "image = F.to_pil_image(new_x_0)\n",
    "plt.imshow(image, cmap='gray')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ec43da92e0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a.4 Training with Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training is mostly the same, but there is one line of change. Before passing our images to our model, we will apply our `random_transforms`. For conveneince, we moved `get_batch_accuracy` to a [utils](./utils.py) file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1715241479297,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "IcgAmvx7rI13",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:23:37.357995Z",
     "start_time": "2025-08-09T02:23:37.351115Z"
    }
   },
   "source": [
    "def train():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        output = model(random_transforms(x))  # Updated\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += utils.get_batch_accuracy(output, y, train_N)\n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hamd, validation remains the same. There are no random transformations. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1715241482250,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "iXc6lnRAR4qZ",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:24:10.889032Z",
     "start_time": "2025-08-09T02:24:10.883310Z"
    }
   },
   "source": [
    "def validate():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += utils.get_batch_accuracy(output, y, valid_N)\n",
    "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put data augmentation to the test. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45384,
     "status": "ok",
     "timestamp": 1715241529445,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "isjOJIVArTLR",
    "outputId": "5d4b6a5f-2ad9-4276-d65e-d84b9874ec3b",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:38:56.056852Z",
     "start_time": "2025-08-09T02:24:41.249058Z"
    }
   },
   "source": [
    "epochs = 20\n",
    "torch._dynamo.config.suppress_errors = True  # B qua li bin dch\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train()\n",
    "    validate()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] WON'T CONVERT inner D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\external_utils.py line 68 \n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] due to: \n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Traceback (most recent call last):\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1213, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     result = self._inner_convert(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 598, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1059, in _compile\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 97, in wrapper_function\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return function(*args, **kwargs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 761, in compile_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 797, in _compile_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     out_code = transform_code_object(code, transform)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1422, in transform_code_object\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     transformations(instructions, code_options)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 257, in _fn\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return fn(*args, **kwargs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 715, in transform\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     tracer.run()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3498, in run\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     super().run()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1337, in run\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     while self.step():\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1246, in step\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3699, in RETURN_VALUE\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self._return(inst)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3684, in _return\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.output.compile_subgraph(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1144, in compile_subgraph\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.compile_and_call_fx_graph(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1437, in compile_and_call_fx_graph\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1487, in call_user_compiler\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._call_user_compiler(gm)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1519, in _call_user_compiler\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 150, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2347, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 2100, in compile_fx\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 760, in _compile_fx_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise InductorError(e, currentframe()).with_traceback(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 745, in _compile_fx_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1295, in fx_codegen_and_compile\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1197, in codegen_and_compile\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = graph.compile_to_module().call\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2083, in compile_to_module\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._compile_to_module()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2091, in _compile_to_module\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                              ^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2002, in codegen\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.scheduler.codegen()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4135, in codegen\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     else self._codegen(self.nodes)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4264, in _codegen\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.get_backend(device).codegen_node(node)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4986, in codegen_node\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3734, in __init__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 418, in pick_vec_isa\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 405, in valid_vec_isa_list\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     isa_list.extend(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 408, in <genexpr>\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                                             ^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 142, in __bool__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.__bool__impl(config.cpp.vec_isa_ok)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 152, in __bool__impl\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.check_build(VecISA._avx_code)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 102, in check_build\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 28, in _get_isa_dry_compile_fingerprint\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 148, in get_cpp_compiler\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     check_compiler_exist_windows(compiler)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 139, in check_compiler_exist_windows\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Traceback (most recent call last):\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1213, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     result = self._inner_convert(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 598, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1059, in _compile\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 97, in wrapper_function\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return function(*args, **kwargs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 761, in compile_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 797, in _compile_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     out_code = transform_code_object(code, transform)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1422, in transform_code_object\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     transformations(instructions, code_options)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 257, in _fn\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return fn(*args, **kwargs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 715, in transform\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     tracer.run()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3498, in run\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     super().run()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1337, in run\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     while self.step():\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1246, in step\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3699, in RETURN_VALUE\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self._return(inst)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3684, in _return\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.output.compile_subgraph(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1144, in compile_subgraph\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.compile_and_call_fx_graph(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1437, in compile_and_call_fx_graph\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1487, in call_user_compiler\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._call_user_compiler(gm)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1519, in _call_user_compiler\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 150, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2347, in __call__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 2100, in compile_fx\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 760, in _compile_fx_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise InductorError(e, currentframe()).with_traceback(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 745, in _compile_fx_inner\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1295, in fx_codegen_and_compile\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1197, in codegen_and_compile\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = graph.compile_to_module().call\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2083, in compile_to_module\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._compile_to_module()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2091, in _compile_to_module\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                              ^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2002, in codegen\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.scheduler.codegen()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4135, in codegen\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     else self._codegen(self.nodes)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4264, in _codegen\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.get_backend(device).codegen_node(node)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4986, in codegen_node\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3734, in __init__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 418, in pick_vec_isa\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 405, in valid_vec_isa_list\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     isa_list.extend(\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 408, in <genexpr>\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                                             ^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 142, in __bool__\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.__bool__impl(config.cpp.vec_isa_ok)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 152, in __bool__impl\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.check_build(VecISA._avx_code)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 102, in check_build\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 28, in _get_isa_dry_compile_fingerprint\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 148, in get_cpp_compiler\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     check_compiler_exist_windows(compiler)\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 139, in check_compiler_exist_windows\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "W0809 09:24:42.798000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] WON'T CONVERT forward C:\\Users\\minhh\\AppData\\Local\\Temp\\ipykernel_16676\\64762065.py line 14 \n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] due to: \n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Traceback (most recent call last):\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1213, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     result = self._inner_convert(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 598, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1059, in _compile\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 97, in wrapper_function\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return function(*args, **kwargs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 761, in compile_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 797, in _compile_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     out_code = transform_code_object(code, transform)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1422, in transform_code_object\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     transformations(instructions, code_options)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 257, in _fn\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return fn(*args, **kwargs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 715, in transform\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     tracer.run()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3498, in run\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     super().run()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1337, in run\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     while self.step():\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1246, in step\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3699, in RETURN_VALUE\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self._return(inst)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3684, in _return\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.output.compile_subgraph(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1144, in compile_subgraph\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.compile_and_call_fx_graph(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1437, in compile_and_call_fx_graph\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1487, in call_user_compiler\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._call_user_compiler(gm)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1519, in _call_user_compiler\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 150, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2347, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 2100, in compile_fx\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 760, in _compile_fx_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise InductorError(e, currentframe()).with_traceback(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 745, in _compile_fx_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1295, in fx_codegen_and_compile\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1197, in codegen_and_compile\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = graph.compile_to_module().call\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2083, in compile_to_module\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._compile_to_module()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2091, in _compile_to_module\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                              ^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2002, in codegen\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.scheduler.codegen()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4135, in codegen\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     else self._codegen(self.nodes)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4264, in _codegen\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.get_backend(device).codegen_node(node)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4986, in codegen_node\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3734, in __init__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 418, in pick_vec_isa\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 405, in valid_vec_isa_list\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     isa_list.extend(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 408, in <genexpr>\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                                             ^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 142, in __bool__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.__bool__impl(config.cpp.vec_isa_ok)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 152, in __bool__impl\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.check_build(VecISA._avx_code)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 102, in check_build\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 28, in _get_isa_dry_compile_fingerprint\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 148, in get_cpp_compiler\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     check_compiler_exist_windows(compiler)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 139, in check_compiler_exist_windows\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Traceback (most recent call last):\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1213, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     result = self._inner_convert(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 598, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1059, in _compile\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 97, in wrapper_function\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return function(*args, **kwargs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 761, in compile_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 797, in _compile_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     out_code = transform_code_object(code, transform)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1422, in transform_code_object\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     transformations(instructions, code_options)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 257, in _fn\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return fn(*args, **kwargs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 715, in transform\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     tracer.run()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3498, in run\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     super().run()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1337, in run\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     while self.step():\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 1246, in step\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3699, in RETURN_VALUE\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self._return(inst)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 3684, in _return\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.output.compile_subgraph(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1144, in compile_subgraph\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.compile_and_call_fx_graph(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1437, in compile_and_call_fx_graph\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1487, in call_user_compiler\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._call_user_compiler(gm)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1519, in _call_user_compiler\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 150, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2347, in __call__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 2100, in compile_fx\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 760, in _compile_fx_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise InductorError(e, currentframe()).with_traceback(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 745, in _compile_fx_inner\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1295, in fx_codegen_and_compile\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1197, in codegen_and_compile\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiled_fn = graph.compile_to_module().call\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2083, in compile_to_module\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self._compile_to_module()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2091, in _compile_to_module\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                              ^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 2002, in codegen\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.scheduler.codegen()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4135, in codegen\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     else self._codegen(self.nodes)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 4264, in _codegen\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.get_backend(device).codegen_node(node)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4986, in codegen_node\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3734, in __init__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 418, in pick_vec_isa\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     _valid_vec_isa_list: list[VecISA] = valid_vec_isa_list()\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 405, in valid_vec_isa_list\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     isa_list.extend(\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 408, in <genexpr>\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                                                             ^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 142, in __bool__\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.__bool__impl(config.cpp.vec_isa_ok)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 152, in __bool__impl\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     return self.check_build(VecISA._avx_code)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 102, in check_build\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 28, in _get_isa_dry_compile_fingerprint\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 148, in get_cpp_compiler\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     check_compiler_exist_windows(compiler)\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]   File \"D:\\PycharmProjects\\NVIDIA-Fundamentals-of-Deep-Learning\\.venv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 139, in check_compiler_exist_windows\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] torch._inductor.exc.InductorError: RuntimeError: Compiler: cl is not found.\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "W0809 09:24:43.203000 16676 .venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1280] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 675.3555 Accuracy: 0.7424\n",
      "Valid - Loss: 75.6869 Accuracy: 0.8813\n",
      "Epoch: 1\n",
      "Train - Loss: 108.2004 Accuracy: 0.9594\n",
      "Valid - Loss: 24.9202 Accuracy: 0.9707\n",
      "Epoch: 2\n",
      "Train - Loss: 57.3018 Accuracy: 0.9787\n",
      "Valid - Loss: 20.8699 Accuracy: 0.9656\n",
      "Epoch: 3\n",
      "Train - Loss: 43.0117 Accuracy: 0.9844\n",
      "Valid - Loss: 39.6900 Accuracy: 0.9334\n",
      "Epoch: 4\n",
      "Train - Loss: 35.9670 Accuracy: 0.9857\n",
      "Valid - Loss: 31.7581 Accuracy: 0.9596\n",
      "Epoch: 5\n",
      "Train - Loss: 30.6269 Accuracy: 0.9886\n",
      "Valid - Loss: 21.9113 Accuracy: 0.9633\n",
      "Epoch: 6\n",
      "Train - Loss: 25.8194 Accuracy: 0.9907\n",
      "Valid - Loss: 19.8029 Accuracy: 0.9700\n",
      "Epoch: 7\n",
      "Train - Loss: 23.0866 Accuracy: 0.9912\n",
      "Valid - Loss: 11.9441 Accuracy: 0.9851\n",
      "Epoch: 8\n",
      "Train - Loss: 18.7527 Accuracy: 0.9930\n",
      "Valid - Loss: 27.1168 Accuracy: 0.9749\n",
      "Epoch: 9\n",
      "Train - Loss: 22.0731 Accuracy: 0.9908\n",
      "Valid - Loss: 12.2645 Accuracy: 0.9856\n",
      "Epoch: 10\n",
      "Train - Loss: 15.1542 Accuracy: 0.9943\n",
      "Valid - Loss: 7.7492 Accuracy: 0.9868\n",
      "Epoch: 11\n",
      "Train - Loss: 18.5210 Accuracy: 0.9932\n",
      "Valid - Loss: 14.3830 Accuracy: 0.9791\n",
      "Epoch: 12\n",
      "Train - Loss: 14.7920 Accuracy: 0.9947\n",
      "Valid - Loss: 8.5657 Accuracy: 0.9792\n",
      "Epoch: 13\n",
      "Train - Loss: 16.0107 Accuracy: 0.9939\n",
      "Valid - Loss: 4.7911 Accuracy: 0.9926\n",
      "Epoch: 14\n",
      "Train - Loss: 14.6157 Accuracy: 0.9945\n",
      "Valid - Loss: 9.3224 Accuracy: 0.9879\n",
      "Epoch: 15\n",
      "Train - Loss: 15.5853 Accuracy: 0.9936\n",
      "Valid - Loss: 13.3465 Accuracy: 0.9844\n",
      "Epoch: 16\n",
      "Train - Loss: 10.7160 Accuracy: 0.9960\n",
      "Valid - Loss: 12.3198 Accuracy: 0.9845\n",
      "Epoch: 17\n",
      "Train - Loss: 12.3166 Accuracy: 0.9950\n",
      "Valid - Loss: 5.8543 Accuracy: 0.9908\n",
      "Epoch: 18\n",
      "Train - Loss: 10.7841 Accuracy: 0.9962\n",
      "Valid - Loss: 2.5091 Accuracy: 0.9962\n",
      "Epoch: 19\n",
      "Train - Loss: 11.7602 Accuracy: 0.9956\n",
      "Valid - Loss: 20.0680 Accuracy: 0.9764\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0WoN84J3Y-l"
   },
   "source": [
    "## Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EPTunxK3Y-l"
   },
   "source": [
    "You will notice that the validation accuracy is higher, and more consistent. This means that our model is no longer overfitting in the way it was; it generalizes better, making better predictions on new data.\n",
    "\n",
    "The training accuracy may be lower, and that's ok. Compared to before, the model is being exposed to a much larger variety of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npYY9cvA3Y-l"
   },
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW_TgWkN3Y-l"
   },
   "source": [
    "Now that we have a well-trained model, we will want to deploy it to perform inference on new images.\n",
    "\n",
    "It is common, once we have a trained model that we are happy with to save it to disk. PyTorch has [multiple ways](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to do this, but for now, we will use `torch.save`. We will also need to save the code for our `MyConvBlock` custom module, which we did in [utils.py](./utils.py). In the next notebook, we'll load the model and use it to read new sign language pictures.\n",
    "\n",
    "PyTorch cannot save a compiled model ([see this post](https://discuss.pytorch.org/t/how-to-save-load-a-model-with-torch-compile/179739)), so we will instead "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1715241533765,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "snAS8LalsMv4",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:40:09.201984Z",
     "start_time": "2025-08-09T02:40:08.948424Z"
    }
   },
   "source": [
    "torch.save(base_model, 'model.pth')"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mtorch\u001B[49m.save(base_model, \u001B[33m'\u001B[39m\u001B[33mmodel.pth\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfePFALr3Y-l"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fo5z3M03Y-l"
   },
   "source": [
    "In this section, you used TorchVision to augment a dataset. This resulted in a trained model with less overfitting and excellent validation image results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgDmGUB93Y-l"
   },
   "source": [
    "### Clear the Memory\n",
    "Before moving on, please execute the following cell to clear up the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R6EXCtGr3Y-l",
    "ExecuteTime": {
     "end_time": "2025-08-09T02:39:13.535662Z",
     "start_time": "2025-08-09T02:39:13.529823Z"
    }
   },
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DIV9ZNW3Y-l"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4iefhaq3Y-l"
   },
   "source": [
    "Now that you have a well-trained model saved to disk, you will, in the next section, deploy it to make predictions on not-yet-seen images.\n",
    "\n",
    "Please continue to the next notebook: [*Model Predictions*](04b_asl_predictions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3I_B1M63Y-l"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
